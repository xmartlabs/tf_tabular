{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens example\n",
    "This example is based on the TFRS movie retrieval example you can find here: https://www.tensorflow.org/recommenders/examples/basic_retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tf_tabular.builder import InputBuilder\n",
    "from tf_tabular.utils import get_vocab\n",
    "from .movielens_model import MovielensModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratings data.\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View dataset examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7]),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 11:13:21.455280: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_genres': array([4]),\n",
      " 'movie_id': b'1681',\n",
      " 'movie_title': b'You So Crazy (1994)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 11:13:21.561915: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in movies.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sampling_probability(all_titles):\n",
    "    unique_movie_titles, movie_counts = np.unique(all_titles, return_counts=True)\n",
    "    total_count = np.sum(movie_counts)\n",
    "    normalized_counts = movie_counts / total_count\n",
    "    sampling_dict = {}\n",
    "    for i, key in enumerate(unique_movie_titles):\n",
    "        sampling_dict[key] = normalized_counts[i]\n",
    "    probs = np.array([sampling_dict[key] for key in all_titles], dtype=np.float32)\n",
    "    return probs, unique_movie_titles\n",
    "\n",
    "def preprocess_ratings(ratings):\n",
    "    ratings = ratings.map(lambda x: {\n",
    "        \"movie_title\": x[\"movie_title\"],\n",
    "        \"movie_genres\": x[\"movie_genres\"],\n",
    "        \"user_id\": x[\"user_id\"],\n",
    "    })\n",
    "\n",
    "    user_ids = ratings.map(lambda x: x[\"user_id\"]).batch(10_000)\n",
    "    unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "    all_titles = ratings.map(lambda x: x[\"movie_title\"]).batch(10_000)\n",
    "    all_titles = np.concatenate(list(all_titles))\n",
    "\n",
    "    probs, unique_movie_titles = compute_sampling_probability(all_titles)\n",
    "\n",
    "    probs = tf.data.Dataset.from_tensor_slices(probs)\n",
    "    ratings = tf.data.Dataset.zip(ratings, probs).map(lambda x, y: dict(x, **{\"sampling_prob\": y}))\n",
    "    return ratings, unique_user_ids, unique_movie_titles\n",
    "\n",
    "\n",
    "def preprocess_movies(movies):\n",
    "    movies = movies.map(lambda x: {\"movie_title\": x[\"movie_title\"],\n",
    "                                \"movie_genres\": x[\"movie_genres\"]\n",
    "                                })\n",
    "\n",
    "    genres = movies.map(lambda x: x[\"movie_genres\"])\n",
    "    unique_movie_genres = np.unique(np.concatenate(list(genres)))\n",
    "    return movies, unique_movie_genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings, unique_user_ids, unique_movie_titles = preprocess_ratings(ratings)\n",
    "movies, unique_movie_genres = preprocess_movies(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model using tf_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = {\"movie_title\": unique_movie_titles,\n",
    "          \"movie_genres\": unique_movie_genres}\n",
    "\n",
    "embedding_dims = {\"movie_title\": 32,\n",
    "                  \"movie_genres\": 32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_builder = InputBuilder()\n",
    "    input_builder.add_inputs_list(categoricals=[\"movie_title\", \"movie_genres\"],\n",
    "                                  vocabs=vocabs,\n",
    "                                  multi_hots=[\"movie_genres\"],\n",
    "                                  embedding_dims=embedding_dims)\n",
    "    inputs, output = input_builder.build_input_layers()\n",
    "    x = Dense(32, activation=None)(output)\n",
    "    return Model(inputs=inputs, outputs=x)\n",
    "\n",
    "movie_model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a simple user model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_user_ids, mask_token=None),\n",
    "  # We add an additional embedding to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now combine both into the two tower MovielensModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "model = MovielensModel(user_model, movie_model)\n",
    "model.prepare_task(movies)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.003))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).ragged_batch(8192).cache()\n",
    "cached_test = test.ragged_batch(4096).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 28s 2s/step - auc: 0.1479 - recall: 0.0013 - factk/top_1_categorical_accuracy: 8.2500e-04 - factk/top_5_categorical_accuracy: 0.0037 - factk/top_100_categorical_accuracy: 0.0743 - loss: 9.6508 - regularization_loss: 0.0000e+00 - total_loss: 9.6508\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 24s 2s/step - auc: 0.1423 - recall: 0.0013 - factk/top_1_categorical_accuracy: 0.0033 - factk/top_5_categorical_accuracy: 0.0147 - factk/top_100_categorical_accuracy: 0.1775 - loss: 9.6240 - regularization_loss: 0.0000e+00 - total_loss: 9.6240\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 24s 2s/step - auc: 0.1341 - recall: 0.0013 - factk/top_1_categorical_accuracy: 0.0040 - factk/top_5_categorical_accuracy: 0.0148 - factk/top_100_categorical_accuracy: 0.2017 - loss: 9.4989 - regularization_loss: 0.0000e+00 - total_loss: 9.4989\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 24s 2s/step - auc: 0.1334 - recall: 0.0013 - factk/top_1_categorical_accuracy: 0.0043 - factk/top_5_categorical_accuracy: 0.0153 - factk/top_100_categorical_accuracy: 0.2220 - loss: 9.2571 - regularization_loss: 0.0000e+00 - total_loss: 9.2571\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 22s 2s/step - auc: 0.1329 - recall: 0.0013 - factk/top_1_categorical_accuracy: 0.0047 - factk/top_5_categorical_accuracy: 0.0222 - factk/top_100_categorical_accuracy: 0.2823 - loss: 9.0336 - regularization_loss: 0.0000e+00 - total_loss: 9.0336\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 22s 2s/step - auc: 0.1323 - recall: 0.0015 - factk/top_1_categorical_accuracy: 0.0060 - factk/top_5_categorical_accuracy: 0.0255 - factk/top_100_categorical_accuracy: 0.3050 - loss: 8.9481 - regularization_loss: 0.0000e+00 - total_loss: 8.9481\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 22s 2s/step - auc: 0.1322 - recall: 0.0023 - factk/top_1_categorical_accuracy: 0.0058 - factk/top_5_categorical_accuracy: 0.0262 - factk/top_100_categorical_accuracy: 0.3127 - loss: 8.8988 - regularization_loss: 0.0000e+00 - total_loss: 8.8988\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 24s 2s/step - auc: 0.1322 - recall: 0.0028 - factk/top_1_categorical_accuracy: 0.0060 - factk/top_5_categorical_accuracy: 0.0274 - factk/top_100_categorical_accuracy: 0.3224 - loss: 8.8609 - regularization_loss: 0.0000e+00 - total_loss: 8.8609\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 24s 2s/step - auc: 0.1323 - recall: 0.0044 - factk/top_1_categorical_accuracy: 0.0064 - factk/top_5_categorical_accuracy: 0.0290 - factk/top_100_categorical_accuracy: 0.3356 - loss: 8.8214 - regularization_loss: 0.0000e+00 - total_loss: 8.8214\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 23s 2s/step - auc: 0.1326 - recall: 0.0052 - factk/top_1_categorical_accuracy: 0.0070 - factk/top_5_categorical_accuracy: 0.0310 - factk/top_100_categorical_accuracy: 0.3518 - loss: 8.7731 - regularization_loss: 0.0000e+00 - total_loss: 8.7731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x295cc0070>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 4s 452ms/step - auc: 0.5013 - recall: 0.0052 - factk/top_1_categorical_accuracy: 0.0074 - factk/top_5_categorical_accuracy: 0.0306 - factk/top_100_categorical_accuracy: 0.3438 - loss: 8.1397 - regularization_loss: 0.0000e+00 - total_loss: 8.1397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'auc': 0.5012891292572021,\n",
       " 'recall': 0.005200000014156103,\n",
       " 'factk/top_1_categorical_accuracy': 0.007350000087171793,\n",
       " 'factk/top_5_categorical_accuracy': 0.030649999156594276,\n",
       " 'factk/top_100_categorical_accuracy': 0.34375,\n",
       " 'loss': 8.058255195617676,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 8.058255195617676}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

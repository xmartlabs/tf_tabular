{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens example\n",
    "This example is based on the TFRS movie retrieval example you can find here: https://www.tensorflow.org/recommenders/examples/basic_retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from tf_tabular.builder import InputBuilder\n",
    "from tf_tabular.utils import get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratings data.\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View dataset examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7]),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 11:13:21.455280: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_genres': array([4]),\n",
      " 'movie_id': b'1681',\n",
      " 'movie_title': b'You So Crazy (1994)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 11:13:21.561915: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in movies.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"movie_genres\": x[\"movie_genres\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "})\n",
    "movies = movies.map(lambda x: {\"movie_title\": x[\"movie_title\"],\n",
    "                               \"movie_genres\": x[\"movie_genres\"]\n",
    "                               })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b\"'Til There Was You (1997)\", b'1-900 (1994)',\n",
       "       b'101 Dalmatians (1996)', b'12 Angry Men (1957)', b'187 (1997)',\n",
       "       b'2 Days in the Valley (1996)',\n",
       "       b'20,000 Leagues Under the Sea (1954)',\n",
       "       b'2001: A Space Odyssey (1968)',\n",
       "       b'3 Ninjas: High Noon At Mega Mountain (1998)',\n",
       "       b'39 Steps, The (1935)'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# movie_titles = movies.map(lambda x: x[\"movie_title\"]).batch(1_000)\n",
    "\n",
    "user_ids = ratings.map(lambda x: x[\"user_id\"]).batch(10_000)\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "all_movies = ratings.map(lambda x: x[\"movie_title\"]).batch(10_000)\n",
    "all_titles = np.concatenate(list(all_movies))\n",
    "unique_movie_titles, movie_counts = np.unique(all_titles, return_counts=True)\n",
    "\n",
    "genres = movies.map(lambda x: x[\"movie_genres\"])\n",
    "unique_movie_genres = np.unique(np.concatenate(list(genres)))\n",
    "\n",
    "display(unique_movie_titles[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = np.sum(movie_counts)\n",
    "normalized_counts = movie_counts / total_count\n",
    "sampling_dict = {}\n",
    "for i, key in enumerate(unique_movie_titles):\n",
    "    sampling_dict[key] = normalized_counts[i]\n",
    "probs = np.array([sampling_dict[key] for key in all_titles], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add sampling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probs = tf.data.Dataset.from_tensor_slices(probs)  #.map(lambda x: tf.cast(x, tf.float32))\n",
    "ratings = tf.data.Dataset.zip(ratings, probs).map(lambda x, y: dict(x, **{\"sampling_prob\": y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MovielensModel(tfrs.Model):\n",
    "    def __init__(self, user_model, movie_model):\n",
    "        super().__init__()\n",
    "        self.movie_model: tf.keras.Model = movie_model\n",
    "        self.user_model: tf.keras.Model = user_model\n",
    "\n",
    "    def prepare_task(self, movies):\n",
    "        id_candidates = (movies.ragged_batch(1024)\n",
    "                         .prefetch(tf.data.AUTOTUNE)\n",
    "                         .cache()\n",
    "                         .map(lambda movie: (movie[\"movie_title\"], self.movie_model(movie))))\n",
    "\n",
    "        metrics = tfrs.metrics.FactorizedTopK(\n",
    "            candidates=tfrs.layers.factorized_top_k.Streaming(k=100).index_from_dataset(id_candidates),\n",
    "            ks=[1,5,100],\n",
    "            name='factk'\n",
    "        )\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True,\n",
    "                                                       reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "        task = tfrs.tasks.Retrieval(\n",
    "            metrics=metrics,\n",
    "            batch_metrics=[tf.keras.metrics.AUC(from_logits=True), tf.keras.metrics.Recall(top_k=10)],\n",
    "            # num_hard_negatives=2,\n",
    "            remove_accidental_hits=True,\n",
    "            loss=loss\n",
    "        )\n",
    "        self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        # We pick out the user features and pass them into the user model.\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        # And pick out the movie features and pass them into the movie model,\n",
    "        # getting embeddings back.\n",
    "        positive_movie_embeddings = self.movie_model({\"movie_title\": features[\"movie_title\"],\n",
    "                                                      \"movie_genres\": features[\"movie_genres\"]\n",
    "                                                      })\n",
    "\n",
    "        # The task computes the loss and the metrics.\n",
    "        return self.task(user_embeddings, positive_movie_embeddings,\n",
    "                         candidate_ids=features[\"movie_title\"],\n",
    "                         candidate_sampling_probability=features[\"sampling_prob\"],\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "vocabs = {\"movie_title\": unique_movie_titles,\n",
    "          \"movie_genres\": unique_movie_genres}\n",
    "\n",
    "embedding_dims = {\"movie_title\": 32,\n",
    "                  \"movie_genres\": 32}\n",
    "\n",
    "def build_model():\n",
    "    input_builder = InputBuilder()\n",
    "    input_builder.add_inputs_list(categoricals=[\"movie_title\", \"movie_genres\"],\n",
    "                                  vocabs=vocabs,\n",
    "                                  multi_hots=[\"movie_genres\"],\n",
    "                                  embedding_dims=embedding_dims)\n",
    "    inputs, output = input_builder.build_input_layers()\n",
    "    x = Dense(32, activation=None)(output)\n",
    "    return Model(inputs=inputs, outputs=x)\n",
    "\n",
    "movie_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_user_ids, mask_token=None),\n",
    "  # We add an additional embedding to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovielensModel(user_model, movie_model)\n",
    "model.prepare_task(movies)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.003))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).ragged_batch(8192).cache()\n",
    "cached_test = test.ragged_batch(4096).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_title': <tf.Tensor: shape=(8192,), dtype=string, numpy=\n",
      "array([b'Godfather, The (1972)', b'Escape to Witch Mountain (1975)',\n",
      "       b'Fargo (1996)', ..., b'Picture Perfect (1997)', b'Fear (1996)',\n",
      "       b'Sleepers (1996)'], dtype=object)>, 'movie_genres': <tf.RaggedTensor [[0, 5, 7], [1, 3, 8], [5, 7, 16], ..., [4, 14], [16], [5, 7]]>, 'user_id': <tf.Tensor: shape=(8192,), dtype=string, numpy=array([b'424', b'429', b'53', ..., b'351', b'551', b'872'], dtype=object)>, 'sampling_prob': <tf.Tensor: shape=(8192,), dtype=float32, numpy=\n",
      "array([0.00413, 0.0003 , 0.00508, ..., 0.00081, 0.00044, 0.00169],\n",
      "      dtype=float32)>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 11:36:11.238956: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for b in cached_train.take(1):\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 35s 3s/step - auc: 0.1477 - recall: 0.0012 - factk/top_1_categorical_accuracy: 5.7500e-04 - factk/top_5_categorical_accuracy: 0.0030 - factk/top_100_categorical_accuracy: 0.0670 - loss: 9.6513 - regularization_loss: 0.0000e+00 - total_loss: 9.6513\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 27s 3s/step - auc: 0.1420 - recall: 0.0014 - factk/top_1_categorical_accuracy: 0.0029 - factk/top_5_categorical_accuracy: 0.0126 - factk/top_100_categorical_accuracy: 0.1734 - loss: 9.6321 - regularization_loss: 0.0000e+00 - total_loss: 9.6321\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 27s 3s/step - auc: 0.1351 - recall: 0.0013 - factk/top_1_categorical_accuracy: 0.0031 - factk/top_5_categorical_accuracy: 0.0145 - factk/top_100_categorical_accuracy: 0.2097 - loss: 9.5279 - regularization_loss: 0.0000e+00 - total_loss: 9.5279\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 27s 3s/step - auc: 0.1334 - recall: 0.0014 - factk/top_1_categorical_accuracy: 0.0033 - factk/top_5_categorical_accuracy: 0.0145 - factk/top_100_categorical_accuracy: 0.2249 - loss: 9.2862 - regularization_loss: 0.0000e+00 - total_loss: 9.2862\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 27s 3s/step - auc: 0.1326 - recall: 0.0013 - factk/top_1_categorical_accuracy: 0.0050 - factk/top_5_categorical_accuracy: 0.0229 - factk/top_100_categorical_accuracy: 0.2850 - loss: 9.0260 - regularization_loss: 0.0000e+00 - total_loss: 9.0260\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 26s 3s/step - auc: 0.1323 - recall: 0.0014 - factk/top_1_categorical_accuracy: 0.0059 - factk/top_5_categorical_accuracy: 0.0265 - factk/top_100_categorical_accuracy: 0.3144 - loss: 8.9196 - regularization_loss: 0.0000e+00 - total_loss: 8.9196\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 27s 3s/step - auc: 0.1322 - recall: 0.0024 - factk/top_1_categorical_accuracy: 0.0063 - factk/top_5_categorical_accuracy: 0.0279 - factk/top_100_categorical_accuracy: 0.3266 - loss: 8.8590 - regularization_loss: 0.0000e+00 - total_loss: 8.8590\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 27s 3s/step - auc: 0.1322 - recall: 0.0030 - factk/top_1_categorical_accuracy: 0.0069 - factk/top_5_categorical_accuracy: 0.0289 - factk/top_100_categorical_accuracy: 0.3356 - loss: 8.8140 - regularization_loss: 0.0000e+00 - total_loss: 8.8140\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 27s 3s/step - auc: 0.1324 - recall: 0.0045 - factk/top_1_categorical_accuracy: 0.0073 - factk/top_5_categorical_accuracy: 0.0307 - factk/top_100_categorical_accuracy: 0.3493 - loss: 8.7677 - regularization_loss: 0.0000e+00 - total_loss: 8.7677\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 26s 3s/step - auc: 0.1329 - recall: 0.0054 - factk/top_1_categorical_accuracy: 0.0076 - factk/top_5_categorical_accuracy: 0.0317 - factk/top_100_categorical_accuracy: 0.3645 - loss: 8.7159 - regularization_loss: 0.0000e+00 - total_loss: 8.7159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x285a80640>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 492ms/step - auc: 0.5015 - recall: 0.0072 - factk/top_1_categorical_accuracy: 0.0066 - factk/top_5_categorical_accuracy: 0.0302 - factk/top_100_categorical_accuracy: 0.3569 - loss: 8.0916 - regularization_loss: 0.0000e+00 - total_loss: 8.0916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'auc': 0.5015000104904175,\n",
       " 'recall': 0.007199999876320362,\n",
       " 'factk/top_1_categorical_accuracy': 0.006649999879300594,\n",
       " 'factk/top_5_categorical_accuracy': 0.03020000085234642,\n",
       " 'factk/top_100_categorical_accuracy': 0.35690000653266907,\n",
       " 'loss': 8.013958930969238,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 8.013958930969238}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
